_____________________________________________________________________________________

Prisma Puppet Project - Beginning Date: 10/11/2014
@Author: Alberto Di Savia Puglisi

This project's aim is to deploy a working openstack environment in one shot.
Of course it's a WORK IN PROGRESS and I really hope it will come to an end!
_____________________________________________________________________________________

DISCLAIMER:
I am using Ubuntu Server 12.04 LTS Precise. Modules should work even 
on other debian-based distros but I am absolutely not sure and I cannot guarantee.
_____________________________________________________________________________________

What does work:
  > Basic puppet settings
  > Hiera and librarian-puppet
  > Galera cluster of MariaDB database servers (3 to 5 nodes)
  > Haproxy load balancers for the Galera cluster (2 to 3 nodes) 
  > HA of these load balancers through Keepalived
  > Openstack Users, DBs and Privileges entries
  > Rabbitmq cluster (3 to 5 nodes)
  > GlusterFS cluster (2 to 5 nodes)
  
What should work better:
  > ssh exec.pp should give the ssh id_rsa to a puppetmaster or 
    something similar and the id_rsa.pub to other nodes 
  > hiera variables are all in one single file
  > you tell me...  

Next steps:
  > Pacemaker and Corosync
  > OpenStack services
  > Keystone
  > ..
_____________________________________________________________________________________

HOW-TO:

  1) Modify the /data/common.yaml file according to your needs.
  Here are some parameter to which you should pay attention:
    
    > galera_nodes -> the number of hosts the haproxy lb and the galera module 
      itself should consider (3, 4 or 5). 4 is the default value.

    > rabbit_nodes -> the number of hosts will form the rabbitmq cluster 
      (3, 4 or 5). 3 is the default value.

    > haproxy_nodes -> the number of haproxy nodes. Choose between 2 and 3.

    > hap1_priority, hap2_priority, hap3_priority -> the priority of each 
      load balancer. 100, 101 and 102 are default values and should be ok.
      
    > gluster_nodes -> the number of glusterfs nodes forming the cluster (2 to 5).
  
    Remember that if you change the number of hosts (galera_nodes, rabbit_nodes,..),
    you should provide at least an equivalent number of relative hostnames and IPs!
  
  2) Edit the /manifests/nodes.pp to meet your requirements:
     > change the hostnames if needed.
  
  3) After you've completed the previous stages, run the following command 
    in order to apply your manifest: sh prisma/scripts/first_apply.sh
  
    This will download and install puppet, ruby and librarian-puppet and execute
    the custom puppet apply script. This script needs to be run just once.
  
    Please ignore the red warning (Warning: Setting templatedir is deprecated...). 
    It is an Ubuntu matter. It appears just once and then it is fixed by puppet. 

  4) Next time you wish to apply the manifest simply use the 'papply' command.
_____________________________________________________________________________________

HOW IT WORKS:

> Galera: in order to create a galera cluster you must firstly apply the manifest
  of the node you chose to be the galera master. It creates the cluster. Then you
  can apply the manifests of other 'slave' nodes.
  
> Keepalived: its decision about what server should take the virtual ip is taken
  considering the sum between two factors: the priority and the weight of
  each server. The weight is equal (by default) for each node but the
  priority is incremental (100 to 102 if there are 3 load balancers).
  Assuming they are only two, the server who gains the ip is the one that has
  the sum of 103 (priority 101 + weight 2) against the other that has sum 102
  (100+2). If the server earning the vip goes down, its weight becomes 0 thus
  his sum becomes 101 (101 + 0) and the less important load balancer (sum 102) 
  takes the vip. When the maior server comes back to life, his weight is 103 
  and it gains the vip again.

> GlusterFS: In order to create a glusterfs remote storage cluster you should 
  firstly apply the manifest of the 'normal' servers (including the 
  glusterfs::server module). Then you can apply the manifest of the main server
  (including the glusterfs::mainserver module). The main server add the peers
  to the cluster, create the volume and start it. Use the client module to 
  mount the mount the GlusterFS volume.
  
_____________________________________________________________________________________

TROUBLESHOOTING:
  
  HAProxy & Keepalived:
    > To try the VIP functionality in addition to the load balancing one, use this:
      mysql -uhaproxy_root -ppassword -h 10.55.1.155 -e "show variables like
      'wsrep_node_name';"
      Remember to change the ip address in the command with your VIP.
    > To understand which Haproxy server keeps the VIP: ip a | grep eth0
    > You can run 2 or 3 HAProxy load balancers but remember to change the 
      haproxy_nodes value in prisma/data/common.yaml 
      
  RabbitMQ cluster:
    > If, for any reason, you need to apply manifest of a rabbitmq node
      a second time, please exec the following command: killall -u rabbitmq
    > To verify the status of your cluster: 'rabbitmqctl cluster_status'
  
  GlusterFS cluster:
    > If the apply action of your mainserver fails you probably have a network issue.
      You most likely should see the red error over the 'gluster peer probe' command.
      This command adds a node to the cluster and if it fails, there is a network
      problem. If you try the command (gluster peer probe NODE_IP) by yourself you 
      should see something like: "peer probe: failed: Probe returned with unknown 
      errno 107". Be sure your nodes can communicate to each other and that you have
      NOT a firewall blocking the glusterfs ports. 
      If you are using IPTABLES, please add these:
   ----------------------------------------------------------------------------------    
   iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 24007:24047 -j ACCEPT 
   iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 111 -j ACCEPT 
   iptables -A INPUT -m state --state NEW -m udp -p udp --dport 111 -j ACCEPT 
   iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 38465:38467 -j ACCEPT
   ----------------------------------------------------------------------------------
      To verify the status of your cluster: 'gluster volume status' from any node
   
   
_____________________________________________________________________________________

If you have any question or suggestion, 
please feel free to contact me at alberto.disavia@ct.infn.it

                                    THAT'S ALL FOLKS
